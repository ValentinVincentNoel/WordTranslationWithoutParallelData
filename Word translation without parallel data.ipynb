{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Challenge\n",
    "# Sébastien Pereira et Valentin Noël\n",
    "# Master DAC\n",
    "# Apprentissage Statistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "L'objectif de l'article est de fournir un modèle d'apprentissage automatique permettant de générer des dictionnaire d'une langue source vers une langue cible dans un paradigme entièrement non supervisé. \n",
    "\n",
    "L'idée principale est de trouver une transformation dans l'espace vectoriel des mots qui aux représentations des mots d'une langue source font correspondre les représentations des mots d'une langue cible. \n",
    "Dans cet article nous supposons que nous avons deux espaces vectoriels correspondant à nos deux ensembles de données, source et cible. Ces espaces correspondent aux embbeding monolingue des langue source et cible obtenus de manière indépendante.\n",
    "\n",
    "L'apprentissage se décompose en deux parties:\n",
    "- Apprentissage adverse : dans cette partie le but est d'obtenir une première approximation de la transformation de l'espace qui définira notre fonction de mapping de la source vers la cible.\n",
    "- Analyse procrustéenne :  dans cette partie nous utilisons quelques points particuiers obtenus à l'étape précédente et nous résolvons un problème de Procuste orthogonal, ces étapes sont répétées de manière itérative jusqu'à convergence.\n",
    "\n",
    "L'inférence s'obtient par la suite grâce à la transformation de l'espace obtenue appliquée aux éléments de la source, puis en utilisant une mesure de similarité CSLS qui permet de résoudre le problème des hubs des KNN (points qui tendent à être les PPV de beaucoup de points en grande dimension -curse of dimensionality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage adverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module, Linear, LeakyReLU, LogSoftmax, NLLLoss\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Module):\n",
    "    def __init__(self, size):\n",
    "        '''\n",
    "        :param size: size of the embbeding space transformation\n",
    "        '''\n",
    "        super(Generator,self).__init__()\n",
    "        self.transformation = Linear(size, size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        :param x: source word representation vector\n",
    "        :return: translated word (vector) according to the generator transformation\n",
    "        '''\n",
    "        return self.transformation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Module):\n",
    "    ''''''\n",
    "    def __init__(self, dim_in, dim_hid, dim_out):\n",
    "        '''\n",
    "        :param dim_in: size of the embbeded space vector\n",
    "        :param dim_hid: size of the hidden layers\n",
    "        :param dim_out: output size (being 2 in our problem either source or target)\n",
    "        '''\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.linear1 = Linear(dim_in, dim_hid)\n",
    "        self.linear2 = Linear(dim_hid, dim_hid)\n",
    "        self.linear3 = Linear(dim_hid, dim_out)\n",
    "        #self.activation3 = Softmax()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        :param x: input word representation (can be eihter transformed source or target)\n",
    "        :return : a score vector of size dim_out\n",
    "        '''\n",
    "        output = LeakyReLU(self.linear1(x))\n",
    "        output = LeakyReLU(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Force prtogonality \n",
    "def orthogonality(matrix, beta):\n",
    "    result = (1+beta) * matrix\n",
    "    result -= beta*(matrix * matrix.T) * matrix \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données\n",
    "\n",
    "Nous allons utiliser les word embbedings monolingues déjà disponibles sur les git de facebook\n",
    "- https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "- https://github.com/facebookresearch/fastText\n",
    "- https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'wiki.fr.vec'\n",
    "with open(filename) as f:\n",
    "    lines = f.readlines()\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [[1,2,1,5,4,6,5],[1,2,1,5,4,6,5]]\n",
    "X = np.array(25*x)\n",
    "y = [[5,4,9,5,2,6,4],[1,2,1,5,4,6,5]]\n",
    "Y = np.array(25*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.DoubleTensor'>\n",
      "\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "    1     2     1     5     4     6     5\n",
      "[torch.DoubleTensor of size 50x7]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "torch.FloatTensor constructor received an invalid combination of arguments - got (torch.LongTensor), but expected one of:\n * no arguments\n * (int ...)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.LongTensor\u001b[0m)\n * (torch.FloatTensor viewed_tensor)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.LongTensor\u001b[0m)\n * (torch.Size size)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.LongTensor\u001b[0m)\n * (torch.FloatStorage data)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.LongTensor\u001b[0m)\n * (Sequence data)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.LongTensor\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-67caea39704a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;34m''' Constructing mini batches '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;31m# 1A Train on target data (real data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0md_real_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0md_real_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_real_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0md_real_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_real_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.FloatTensor constructor received an invalid combination of arguments - got (torch.LongTensor), but expected one of:\n * no arguments\n * (int ...)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.LongTensor\u001b[0m)\n * (torch.FloatTensor viewed_tensor)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.LongTensor\u001b[0m)\n * (torch.Size size)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.LongTensor\u001b[0m)\n * (torch.FloatStorage data)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.LongTensor\u001b[0m)\n * (Sequence data)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.LongTensor\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "emb_space_size = 300\n",
    "discriminator_hid_size = 2048\n",
    "output_size = 2\n",
    "\n",
    "G = Generator(emb_space_size)\n",
    "D = Discriminator(emb_space_size, discriminator_hid_size, output_size)\n",
    "\n",
    "m = LogSoftmax()\n",
    "L = NLLLoss()\n",
    "\n",
    "d_optimizer = torch.optim.SGD(D.parameters(), lr=0.1, weight_decay=0.95)\n",
    "g_optimizer = torch.optim.SGD(G.parameters(), lr=0.1, weight_decay=0.95)\n",
    "\n",
    "\"\"\"========================== ADVERSARIAL TRAINING ==================================\"\"\"\n",
    "\n",
    "X = torch.from_numpy(X.astype(float))\n",
    "Y = torch.from_numpy(Y)\n",
    "\n",
    "num_epochs = 3\n",
    "bs = 26 # batch_size\n",
    "d_nb_batches = int(len(Y) / bs)\n",
    "g__nb_batches = int(len(X) / bs)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "            for j in range(nb_batches):\n",
    "                ''' 1 Train discriminator on source+target data '''\n",
    "                D.zero_grad()\n",
    "                \n",
    "                ''' Constructing mini batches '''\n",
    "                # 1A Train on target data (real data)\n",
    "                d_real_data = Variable(torch.FloatTensor(Y[j * bs:(j + 1) * bs, :]))\n",
    "                d_real_prediction = D(d_real_data)\n",
    "                d_real_error = L(m(d_real_prediction), Variable(torch.zeros(bs)))\n",
    "                d_real_error.backward()\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                # 1B Train on source data (fake data)\n",
    "                d_gen_input = Variable(torch.FloatTensor(X[j * bs:(j + 1) * bs, :]))\n",
    "                d_fake_data = G(d_gen_input).detach()\n",
    "                d_fake_prediction = D(d_fake_data)\n",
    "                d_fake_error = L(m(d_fake_prediction), Variable(torch.ones(bs)))\n",
    "                d_fake_error.backward()\n",
    "                d_optimizer.step()\n",
    "           \n",
    "                ''' 2 Train generator on D but do not train D here'''\n",
    "                G.zero_grad()\n",
    "                \n",
    "                gen_input = Variable(torch.FloatTensor(X[j * bs:(j + 1) * bs, :]))\n",
    "                d_fake_data = G(gen_input)\n",
    "                dg_fake_prediction = D(d_fake_data)\n",
    "                g_error = L(m(dg_fake_prediction), Variable(torch.ones(bs)))\n",
    "                g_error.backward()\n",
    "                g_optimizer.step()\n",
    "                \n",
    "                # Orthogonality\n",
    "                G.parameters = orthogonality(G.parameters, 0.01)\n",
    "                \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
